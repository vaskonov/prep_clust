{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import scipy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.metrics.cluster import contingency_matrix \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(texts, model, tokenizer, prep, mask=False):\n",
    "    \n",
    "    embs = []\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "               \n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)[:512]\n",
    "        \n",
    "        attention_ids = [1] * len(indexed_tokens)\n",
    "        segments_ids = [0] * len(indexed_tokens)\n",
    "        \n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        attention_tensors = torch.tensor([attention_ids])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "#             outputs = model(input_ids=tokens_tensor, attention_mask=attention_tensors, token_type_ids=segments_tensors, return_dict=True)\n",
    "            outputs = model(input_ids=tokens_tensor, attention_mask=attention_tensors, return_dict=True, output_hidden_states=True)\n",
    "            hidden_states = outputs.last_hidden_state      \n",
    "\n",
    "            if prep in tokenized_text:\n",
    "                token_idx = tokenized_text.index(prep)\n",
    "                sum_vec = hidden_states[0][token_idx]\n",
    "                embs.append(sum_vec.numpy().tolist())\n",
    "            else:\n",
    "                print(prep, 'not found in', marked_text, text)\n",
    "                embs.append(float(\"nan\"))\n",
    "                continue\n",
    "            \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPOSITION = 'в силу' \n",
    "ENCODER = 'DeepPavlov/rubert-base-cased'\n",
    "PREPOSITION_CONTENT = PREPOSITION.split()[1] #силу\n",
    "# ENCODER = 'bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygsheets\n",
    "c = pygsheets.authorize(service_file='client_secret.json')\n",
    "# ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(ENCODER)\n",
    "encoder = BertModel.from_pretrained(ENCODER, output_hidden_states = True)\n",
    "\n",
    "# make sure that the prep is tokenized properly (no subtokens)\n",
    "assert len(tokenizer(PREPOSITION_CONTENT).input_ids) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh.worksheets() #list all preps in a file\n",
    "wk = sh.worksheet_by_title(PREPOSITION)\n",
    "df = wk.get_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing if needed\n",
    "\n",
    "# df.dropna(subset=['year'],inplace=True)\n",
    "# df['year'] = df['year'].str[:4]\n",
    "# df['year_split'] = df['year'].astype(str).str.split(pat=\"-\").str[0]\n",
    "# df['text_cleaned'] = df['text'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "# df['text_cleaned'] = df['text_cleaned'].astype(str).str.lower()\n",
    "# df['year'] = df['year'].astype('int32')\n",
    "# df = df[df['year'].str.isdigit()]\n",
    "\n",
    "df['text'] = df['text'].astype(str).str.lower()\n",
    "df['year'] = df['year'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embs'] = get_embedding(df[\"text\"], encoder, tokenizer, PREPOSITION_CONTENT, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['year', 'text', 'embs'], inplace=True)\n",
    "df.to_pickle(\"./pkl/\"+PREPOSITION+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "stats = {}\n",
    "\n",
    "for prep in [PREPOSITION]:\n",
    "    \n",
    "    stats[prep] = {}\n",
    "    print(\"prep\", prep)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_pickle(\"pkl/\"+prep+\".pkl\")\n",
    "        print(df.shape)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    for year in [(1800,1850), (1850, 1900), (1900, 1950), (1950, 2000)]:\n",
    "        print(\"period\", year)\n",
    "    \n",
    "        kmeans = {}\n",
    "        silhouette_list = []\n",
    "        \n",
    "        small_df = df[df['year'].between(year[0], year[1])]\n",
    "        print(\"Shape of the defined period:\", small_df.shape)\n",
    "        \n",
    "        X = small_df['embs'].tolist()\n",
    "\n",
    "        for n_clusters in range(2, 7):\n",
    "            try:\n",
    "                clusterer = KMeans(n_clusters=n_clusters, random_state=10).fit(X)\n",
    "            except:\n",
    "                continue  \n",
    "                \n",
    "            cluster_labels = clusterer.predict(X)\n",
    "            kmeans[n_clusters] = clusterer\n",
    "\n",
    "            silhouette = silhouette_score(X, cluster_labels)\n",
    "            silhouette_list.append((n_clusters, silhouette))\n",
    "\n",
    "        silhouette_list_sorted = sorted(silhouette_list, key=lambda x: x[1], reverse=True)\n",
    "        stats[prep][year] = silhouette_list_sorted\n",
    "\n",
    "        \n",
    "        small_df.assign(cluster=lambda x: kmeans[silhouette_list_sorted[0][0]].predict(x.embs.tolist())[0])\n",
    "#         small_df['cluster'] = small_df['embs'].apply(lambda x: kmeans[silhouette_list_sorted[0][0]].predict([x])[0])\n",
    "#         small_df['cluster_distance'] = small_df['embs'].apply(lambda x: [scipy.spatial.distance.euclidean(cl, x) for cl in kmeans[silhouette_list_sorted[0][0]].cluster_centers_])\n",
    "    \n",
    "        new_df = pd.concat([new_df, small_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'в силу': {(1800, 1850): [(2, 0.16734242536917696),\n",
       "   (4, 0.1076494598462531),\n",
       "   (6, 0.08846447846254077),\n",
       "   (5, 0.0709829133248713),\n",
       "   (3, 0.06871632535788721)],\n",
       "  (1850, 1900): [(2, 0.07675602386730403),\n",
       "   (3, 0.06588214788759043),\n",
       "   (4, 0.06142597883234597),\n",
       "   (6, 0.04178164581758671),\n",
       "   (5, 0.039427527694123145)],\n",
       "  (1900, 1950): [(2, 0.09444822894245805),\n",
       "   (3, 0.06976714339675091),\n",
       "   (4, 0.055987144184087505),\n",
       "   (6, 0.052077270356110386),\n",
       "   (5, 0.047939452056141564)],\n",
       "  (1950, 2000): [(2, 0.16142255796431365),\n",
       "   (3, 0.14147452958160067),\n",
       "   (4, 0.088486379816195),\n",
       "   (5, 0.07130788867854179),\n",
       "   (6, 0.06728665077453826)]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.sort_values(by=['year'], inplace=True)\n",
    "new_df.drop('embs', axis='columns', inplace=True)\n",
    "\n",
    "wks = sh.worksheet_by_title(\"=\"+PREPOSITION)\n",
    "wks.set_dataframe(new_df,(1,1), fit=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
